""" This module generates notes for a midi file using the
    trained neural network """
import pickle
import numpy
from music21 import instrument, note, stream, chord, converter, environment
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import LSTM
from keras.layers import BatchNormalization as BatchNorm
from keras.layers import Activation
environment.set('musescoreDirectPNGPath', 'C:\\Program Files\\MuseScore 3\\bin\\MuseScore3.exe')

def generate():
    """ Generates a basic piano midi file for use within the code, AKA the base composition to work off of """
    #load the notes used to train the model, saved in the data folder as an rb file
    with open('data/notes', 'rb') as filepath:
        notes = pickle.load(filepath)

    # Get all pitch names
    pitchnames = sorted(set(item for item in notes))
    # Get all pitch names
    n_vocab = len(set(notes))

    network_input, normalized_input = prepare_sequences(notes, pitchnames, n_vocab)
    model = create_network(normalized_input, n_vocab)
    prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)
    create_midi(prediction_output)
    adjust_midi()
    print("All functions have been ran correctly.")

def prepare_sequences(notes, pitchnames, n_vocab):
    """ Prepare the sequences used by the Neural Network """
    # map between notes and integers and back
    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))
    
    
    sequence_length = 100
    network_input = []
    output = []
    for i in range(0, len(notes) - sequence_length, 1):
        sequence_in = notes[i:i + sequence_length]
        sequence_out = notes[i + sequence_length]
        network_input.append([note_to_int[char] for char in sequence_in])
        output.append(note_to_int[sequence_out])

    n_patterns = len(network_input)

    # reshape the input into a format compatible with LSTM layers
    normalized_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))
    # normalize input
    normalized_input = normalized_input / float(n_vocab)

    return (network_input, normalized_input)
    
    print("The prepare_sequences function has succesfully ran.")

def create_network(network_input, n_vocab):
    """ create the structure of the neural network used to generate the MIDI """
    model = Sequential()
    model.add(LSTM(
        512,
        input_shape=(network_input.shape[1], network_input.shape[2]),
        recurrent_dropout=0.3,
        return_sequences=True
    ))
    model.add(LSTM(512, return_sequences=True, recurrent_dropout=0.3,))
    model.add(LSTM(512))
    model.add(BatchNorm())
    model.add(Dropout(0.3))
    model.add(Dense(256))
    model.add(Activation('relu'))
    model.add(BatchNorm())
    model.add(Dropout(0.3))
    model.add(Dense(n_vocab))
    model.add(Activation('softmax'))
    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')

    # Load the weights to each node
    model.load_weights('weights-retrained.hdf5')

    return model
    print("The create_network function has succesfully ran.")

def generate_notes(model, network_input, pitchnames, n_vocab):
    """ Generate notes from the neural network based on a sequence of notes """
    # pick a random sequence from the input as a starting point for the prediction
    start = numpy.random.randint(0, len(network_input)-1)

    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))

    pattern = network_input[start]
    prediction_output = []

    # generate 500 notes
    for note_index in range(500):
        prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))
        prediction_input = prediction_input / float(n_vocab)

        prediction = model.predict(prediction_input, verbose=0)

        index = numpy.argmax(prediction)
        result = int_to_note[index]
        prediction_output.append(result)

        pattern.append(index)
        pattern = pattern[1:len(pattern)]

    return prediction_output
    print("The generate notes function has succesfully ran")
    
def create_midi(prediction_output):
    """ convert the output from the prediction to notes and create a midi file
        from the notes """
    offset = 0
    output_notes = []

    # create note and chord objects based on the values generated by the model
    for pattern in prediction_output:
        # pattern is a chord
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:
                new_note = note.Note(int(current_note))
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
        # pattern is a note
        else:
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)

        # increase offset each iteration so that notes do not stack
        offset += 0.5

    midi_stream = stream.Stream(output_notes)

    midi_stream.write('midi', fp='test_output.mid')
    print("Midi has been created via the create_midi function")

def adjust_midi():
    """Converts the scale of previously output midi file to tempo based upon heartrate.
       standard output of midi stream is 120bpm, scale of 0.5 = 60bpm (heartrate) 
       Changes instruments used based upon a weather input, transposes key dependent on weather and heartrate"""
    
    #heart rate can be changed here to adjust scale/bpm of track - futureworks - live reading of heart rate
    heart_BPM = 80
    
    #the weather can be set to either rain or sun for a different output - futureworks - live feed of weather
    weather = 'rain'
    
    #defines a value for scale and pitch to adjust the track, 1 being kept the same, 0.5 speeds up, 2 slows down
    scale_BPM = 1
    
    #parses the song previously output by create_midi function
    midi_stream_adjusted = converter.parse('test_output.mid')
    #sees what key the midi is in, then sets the pitch to the key defined by if statement
    song_Key = midi_stream_adjusted.analyze('key')
    print('the song is in the key: ', song_Key)
    
    #changes how song is written depending on heart rate and weather values
    if 60 <= heart_BPM <= 100 and weather == 'Rain' or 'rain':
        scale_BPM = 0.5
        
        #changes the instrument used within the song, rain uses harp, sun uses a vibraphone, default is piano
        for el in midi_stream_adjusted.recurse():
            if 'Instrument' in el.classes: 
                el.activeSite.replace(el, instrument.Harp())
                
        #transposes the key of the song   
        new_BPM_midi = midi_stream_adjusted.parts[0].pitches[0]
        new_BPM_midi.transpose(1, inPlace=True)
        
        #adjusts the tempo of the song
        new_BPM_midi = midi_stream_adjusted.augmentOrDiminish(scale_BPM)
        
        #writes new midi file with key and tempo change, both files kept for comparisson.
        new_BPM_midi.write('midi', 'tempo_change_output_resting_rain.mid')
        new_BPM_score = converter.parse('tempo_change_output_resting_rain.mid')
        new_BPM_score.show()
        
        print("A song has been output that correlates with the input of a resting heartrate and rainy weather.")
        
    elif 60 <= heart_BPM <= 100 and weather == 'Sun' or 'sun':
        scale_BPM = 0.5
        
        #changes the instrument used within the song, rain uses harp, sun uses a vibraphone, default is piano
        for el in midi_stream_adjusted.recurse():
            if 'Instrument' in el.classes: 
                el.activeSite.replace(el, instrument.Vibraphone())
                
        #transposes the key of the song   
        new_BPM_midi = midi_stream_adjusted.parts[0].pitches[0]
        new_BPM_midi.transpose(1, inPlace=True)
        
        #adjusts the tempo of the song
        new_BPM_midi = midi_stream_adjusted.augmentOrDiminish(scale_BPM)
        
        #writes new midi file with key and tempo change, both files kept for comparisson.
        new_BPM_midi.write('midi', 'tempo_change_output_resting_sun.mid')
        new_BPM_score = converter.parse('tempo_change_output_resting_sun.mid')
        new_BPM_score.show()
        
        print("A song has been output that correlates with the input of a resting heartrate and sunny weather.")
        
    elif heart_BPM > 100  and weather == 'Rain' or 'rain':
        scale_BPM = 2
        
        #changes the instrument used within the song, rain uses harp, sun uses a vibraphone, default is piano
        for el in midi_stream_adjusted.recurse():
            if 'Instrument' in el.classes: 
                el.activeSite.replace(el, instrument.Harp())
                
        #transposes the key of the song   
        new_BPM_midi = midi_stream_adjusted.parts[0].pitches[0]
        new_BPM_midi.transpose(10, inPlace=True)
        
        #adjusts the tempo of the song
        new_BPM_midi = midi_stream_adjusted.augmentOrDiminish(scale_BPM)
        #writes new midi file with key and tempo change, both files kept for comparisson.
        new_BPM_midi.write('midi', 'tempo_change_output_active_rain.mid')
        new_BPM_score = converter.parse('tempo_change_output_active_rain.mid')
        new_BPM_score.show()
        print("A song has been output that correlates with the input of an active heartrate and rainy weather.")
        
    elif heart_BPM > 100  and weather == 'Sun' or 'sun':
        scale_BPM = 2
          
        #changes the instrument used within the song, rain uses harp, sun uses a vibraphone, default is piano
        for el in midi_stream_adjusted.recurse():
            if 'Instrument' in el.classes: 
                el.activeSite.replace(el, instrument.Vibraphone())
                
        #transposes the key of the song   
        new_BPM_midi = midi_stream_adjusted.parts[0].pitches[0]
        new_BPM_midi.transpose(10, inPlace=True)
        
        #adjusts the tempo of the song
        new_BPM_midi = midi_stream_adjusted.augmentOrDiminish(scale_BPM)
        
        #writes new midi file with key and tempo change, both files kept for comparisson.
        new_BPM_midi.write('midi', 'tempo_change_output_active_sun.mid')
        new_BPM_score = converter.parse('tempo_change_output_active_sun.mid')
        new_BPM_score.show()
        
        #statement to say that the function has ran
        print("A song has been output that correlates with the input of an active heartrate and sunny weather.")
    else:
        scale_BPM = 1
        
        #changes the instrument used within the song, rain uses harp, sun uses a vibraphone, default is piano
        for el in midi_stream_adjusted.recurse():
            if 'Instrument' in el.classes: 
                el.activeSite.replace(el, instrument.Piano())
                
        #transposes the key of the song   
        new_BPM_midi = midi_stream_adjusted.parts[0].pitches[0]
        new_BPM_midi.transpose(1, inPlace=True)
        
        #adjusts the tempo of the song
        new_BPM_midi = midi_stream_adjusted.augmentOrDiminish(scale_BPM)
     
        #writes new midi file with key and tempo change, both files kept for comparisson.
        new_BPM_midi.write('midi', 'tempo_change_output_no_changes.mid')
        new_BPM_score = converter.parse('tempo_change_output_no_changes.mid')
        new_BPM_score.show()
        print("Either the heartrate was below 60 (resting heartrate) or weather was misspelled, an unedited song has been output instead")
    
    
    #clause to initialise all the functions within the file
if __name__ == '__main__':
    generate()
